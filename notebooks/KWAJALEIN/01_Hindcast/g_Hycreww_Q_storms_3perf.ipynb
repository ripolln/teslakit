{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "... ***CURRENTLY UNDER DEVELOPMENT*** ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyCReeW-Q : Overtopping estimation of historical nearshore storms at 3 locations\n",
    "\n",
    "inputs required: \n",
    "  * Historical DWTs\n",
    "  * MU - TAU intradaily hidrographs time series\n",
    "  * Historical water levels\n",
    "\n",
    "in this notebook:\n",
    "  * Transform storms into hourly time series\n",
    "  * HyCReWW-Q overtopping estimation of historical events\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# common\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# pip\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from datetime import datetime\n",
    "\n",
    "# DEV: override installed teslakit\n",
    "import sys\n",
    "sys.path.insert(0, op.join(os.path.abspath(''), '..', '..', '..'))\n",
    "\n",
    "# teslakit\n",
    "from teslakit.database import Database\n",
    "from teslakit.rbf import RBF_Interpolation\n",
    "from teslakit.mda import Normalize\n",
    "\n",
    "from teslakit.util.time_operations import npdt64todatetime, fast_reindex_hourly, repair_times_hourly, add_max_storms_mask\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Database and Site parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'zBeach' ()>\n",
      "array(1.473809)\n",
      "Coordinates:\n",
      "    profile  int64 28\n",
      "<xarray.DataArray 'zBeach' ()>\n",
      "array(1.533132)\n",
      "Coordinates:\n",
      "    profile  int64 8\n",
      "<xarray.DataArray 'zBeach' ()>\n",
      "array(1.182817)\n",
      "Coordinates:\n",
      "    profile  int64 16\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Teslakit database\n",
    "\n",
    "p_data = r'/Users/albacid/Projects/TeslaKit_projects'\n",
    "db = Database(p_data)\n",
    "\n",
    "# set site\n",
    "db.SetSite('KWAJALEIN')\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# N, S, E profiles\n",
    "pN = 28 #(36,22)\n",
    "pS = 8  #(13,1)\n",
    "pE = 16 #(21,14)\n",
    "#pN = 35 #28 #(36,22)\n",
    "#pS = 4  #8  #(13,1)\n",
    "#pE = 20 #16 #(21,14)\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# Load historical TWL\n",
    "hist_TWL = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/TIDE/hist_TWL.nc'))\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# load Hycreww RBF coefficients and sim. variables min. and max.\n",
    "\n",
    "# reef characteristics (AlbaR)\n",
    "xds = xr.open_dataset(op.join(p_data, 'inputs_Kwajalein','datos_perfiles_albaR','data_profiles.nc'))\n",
    "xds = xds.drop('zBeach')\n",
    "reef_N = xds.sel(profile = pN )\n",
    "reef_S = xds.sel(profile = pS)\n",
    "reef_E = xds.sel(profile = pE)\n",
    "\n",
    "\n",
    "# Zbeach characteristics (Sara)\n",
    "xds_Zbeach = xr.open_dataset(op.join(p_data, 'inputs_Kwajalein','datos_perfiles_sara','zBeach_clean_P1.nc'))\n",
    "#xds_Zbeach = xr.open_dataset(op.join(p_data, 'inputs_Kwajalein','datos_perfiles_sara','zBeach_perimeter1.nc'))\n",
    "xds_Zbeach = xds_Zbeach.drop({'xcoor', 'ycoor'})\n",
    "\n",
    "Zbeach_N = xds_Zbeach.sel(profile = pN )\n",
    "Zbeach_S = xds_Zbeach.sel(profile = pS)\n",
    "Zbeach_E = xds_Zbeach.sel(profile = pE)\n",
    "print(Zbeach_N.zBeach)\n",
    "print(Zbeach_S.zBeach)\n",
    "print(Zbeach_E.zBeach)\n",
    "sys.exit()\n",
    "\n",
    "# Hycreww limits and coefficients\n",
    "var_lims, rbf_coeffs = db.Load_HYCREWW_Q()\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# MU - TAU intradaily hidrographs time series\n",
    "xds_mutau_N = xr.open_dataset(op.join(p_data, 'sites', 'KWAJALEIN', 'ESTELA', 'hydrographs', 'MuTau_N.nc'))\n",
    "xds_mutau_S = xr.open_dataset(op.join(p_data, 'sites', 'KWAJALEIN', 'ESTELA', 'hydrographs', 'MuTau_S.nc'))\n",
    "xds_mutau_E = xr.open_dataset(op.join(p_data, 'sites', 'KWAJALEIN', 'ESTELA', 'hydrographs', 'MuTau_E.nc'))\n",
    "\n",
    "#xds_mutau_N = xds_mutau_N.drop('MU')\n",
    "#xds_mutau_S = xds_mutau_S.drop('MU')\n",
    "#xds_mutau_E = xds_mutau_E.drop('MU')\n",
    "print(xds_mutau_N)\n",
    "\n",
    "#xds_mutau_N = xds_mutau_N.rename({'MU2':'MU'})\n",
    "#xds_mutau_S = xds_mutau_S.rename({'MU2':'MU'})\n",
    "#xds_mutau_E = xds_mutau_E.rename({'MU2':'MU'})\n",
    "#print(xds_mutau_N)\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# data for plotting comparison\n",
    "WAVES_N_h = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/WAVES/swells_profiles_N.nc')) # hourly\n",
    "WAVES_S_h = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/WAVES/swells_profiles_S.nc')) # 3-hourly\n",
    "WAVES_E_h = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/WAVES/swells_profiles_E.nc')) # 3-hourly\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# data for plotting comparison\n",
    "WT = xr.open_dataset('/Users/albacid/Projects/TeslaKit_projects/sites/KWAJALEIN/ESTELA/pred_SLP/kma.nc')\n",
    "print()\n",
    "print(WT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storms to hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intradaily_Hydrograph(xds_mutau):\n",
    "    '''\n",
    "    Calculates intradaily hydrograph (hourly) from a time series of storms.\n",
    "    #storms waves data (hs, tp, dir) and (mu, tau, ss) is needed.\n",
    "\n",
    "    xds_wvs (waves aggregated):\n",
    "        xarray.Dataset (time,), Hs, Tp, Dir\n",
    "\n",
    "    xds_tcs (TCs):\n",
    "        xarray.Dataset (time,), mu, tau, ss\n",
    "\n",
    "    returns xarray.Dataset (time,), Hs, Tp, Dir, SS  (hourly)\n",
    "    '''\n",
    "\n",
    "    # input data (storms aggregated waves)\n",
    "    Hs = xds_mutau.Hs.values[:]\n",
    "    Tp = xds_mutau.Tp.values[:]\n",
    "    Dir = xds_mutau.Dir.values[:]\n",
    "    ts = xds_mutau.time.values[:]    \n",
    "    \n",
    "    \n",
    "    # TODO: this should not be needed\n",
    "    if isinstance(ts[0], np.datetime64):\n",
    "        ts = [npdt64todatetime(x) for x in ts]\n",
    "        \n",
    "\n",
    "    # input data (storms TCs)\n",
    "    tau = xds_mutau.TAU.values[:]  # storm max. instant (0-1)\n",
    "    mu = xds_mutau.MU.values[:]\n",
    "    #ss = xds_tcs.ss.values[:]\n",
    "    \n",
    "        \n",
    "    # storm durations\n",
    "    s_dur_h = np.array([x.total_seconds()/3600 for x in np.diff(ts)])  # hours    \n",
    "    s_cs_h = np.cumsum(s_dur_h)  # hours since time start\n",
    "    s_cs_h = np.insert(s_cs_h,0,0)    \n",
    "\n",
    "    \n",
    "    # storm tau max (hourly)\n",
    "    tau_h = np.floor(s_cs_h[:-1] + s_dur_h * tau[:-1])\n",
    "    \n",
    "    \n",
    "    # aux function\n",
    "    def CalcHydro(vv, vt, tt, mt):\n",
    "        '''\n",
    "        Calculate variable hourly hydrograph.\n",
    "        vv - var value at max.\n",
    "        vt - var time (hours since start, at hydrograph extremes)\n",
    "        tt - tau max time (hours since start).\n",
    "        mt - mu value\n",
    "        '''\n",
    "        \n",
    "        # var value at hydrographs extremes\n",
    "        vv_extr = vv * np.power(2*mt-1, 2)\n",
    "\n",
    "        \n",
    "        # make it continuous\n",
    "        vv_extr_cont = (np.roll(vv_extr,1) + vv_extr) / 2\n",
    "        vv_extr_cont[0] = vv_extr_cont[1]\n",
    "        vv_extr_cont[-1] = vv_extr_cont[-2]\n",
    "        \n",
    "        \n",
    "        # join hydrograph max. and extremes variable data\n",
    "        vt_full = np.concatenate([vt, tt])  # concatenate times (used for sorting)\n",
    "        vv_full = np.concatenate([vv_extr_cont, vv])        \n",
    "\n",
    "        \n",
    "        # sort data\n",
    "        ix = np.argsort(vt_full)\n",
    "        vt_sf = vt_full[ix]\n",
    "        vv_sf = vv_full[ix]\n",
    "        \n",
    "            \n",
    "        # interpolate to fill all hours\n",
    "        h_times = np.arange(vt_sf[0], vt_sf[-1] + 1, 1)\n",
    "        \n",
    "        h_values = np.interp(h_times, vt_sf, vv_sf)\n",
    "\n",
    "        # fix times\n",
    "        h_times = h_times.astype(int)\n",
    "        \n",
    "\n",
    "        return h_values, h_times\n",
    "    \n",
    "\n",
    "    # hydrograph variables: hs and ss\n",
    "    hourly_Hs, hourly_times = CalcHydro(Hs, s_cs_h, tau_h, mu)        \n",
    "    #hourly_ss, _ = CalcHydro(ss, s_cs_h, tau_h, mu)\n",
    "    \n",
    "    \n",
    "    # resample waves data to hourly (pad Tp and Dir)\n",
    "    xds_wvs_h = fast_reindex_hourly(xds_mutau)\n",
    "            \n",
    "    # select wave variables\n",
    "    xds_wvs_h = xds_wvs_h[['Hs','Tp','Dir']]\n",
    "\n",
    "    # add Hs and SS \n",
    "    xds_wvs_h['Hs'] =(('time',), hourly_Hs)\n",
    "    #xds_wvs_h['SS'] =(('time',), hourly_ss)\n",
    "\n",
    "    return xds_wvs_h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate intradaily hourly hydrographs for simulated storms\n",
    "WAVES_N = Intradaily_Hydrograph(xds_mutau_N)\n",
    "WAVES_S = Intradaily_Hydrograph(xds_mutau_S)\n",
    "WAVES_E = Intradaily_Hydrograph(xds_mutau_E)\n",
    "print(WAVES_N)\n",
    "print()\n",
    "\n",
    "# repair times: remove duplicates (if any)\n",
    "WAVES_N = repair_times_hourly(WAVES_N)\n",
    "WAVES_S = repair_times_hourly(WAVES_S)\n",
    "WAVES_E = repair_times_hourly(WAVES_E)\n",
    "print(WAVES_N)\n",
    "print()\n",
    "\n",
    "# add mask for max_storms times\n",
    "#WVS_h = add_max_storms_mask(WVS_h, xds_mutau_N.Runup_max.values, name_mask='max_storms')\n",
    "#print(WVS_h)\n",
    "#print()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all profiles (1 year zoom)\n",
    "\n",
    "WAVES_N_h = WAVES_N_h.sel(profile=pN)\n",
    "WAVES_S_h = WAVES_S_h.sel(profile=pS)\n",
    "WAVES_E_h = WAVES_E_h.sel(profile=pE)\n",
    "\n",
    "\n",
    "# plot N\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_N_h.time, WAVES_N_h.hs, label='Hs original waves')\n",
    "plt.plot(WAVES_N.time, WAVES_N.Hs,color='C1',label='Hs h from storms')\n",
    "plt.plot(xds_mutau_N.time_runup_max, xds_mutau_N.Hs, '*r',label='Hs storms')\n",
    "plt.legend()\n",
    "plt.xlim(datetime(1997,1,1),datetime(1998,1,1))\n",
    "plt.title('North profile. pN = ' + str(pN))\n",
    "\n",
    "\n",
    "# plot S\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_S_h.time, WAVES_S_h.hs, label='Hs original waves')\n",
    "plt.plot(WAVES_S.time, WAVES_S.Hs, color='C1',label='Hs h from storms')\n",
    "plt.plot(xds_mutau_S.time_runup_max, xds_mutau_S.Hs, '*r', label='Hs storms')\n",
    "plt.legend()\n",
    "plt.xlim(datetime(1997,1,1),datetime(1998,1,1))\n",
    "plt.title('South profile. pS = ' + str(pS))\n",
    "\n",
    "\n",
    "# plot E\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_E_h.time, WAVES_E_h.hs, label='Hs original waves')\n",
    "plt.plot(WAVES_E.time, WAVES_E.Hs, color='C1',label='Hs h from storms')\n",
    "plt.plot(xds_mutau_E.time_runup_max, xds_mutau_E.Hs, '*r', label='Hs storms')\n",
    "plt.legend()\n",
    "plt.xlim(datetime(1997,1,1),datetime(1998,1,1))\n",
    "plt.title('East profile. pE = ' + str(pE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot South profile & WTs (1 month zoom)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_S_h.time, WAVES_S_h.hs, label='Hs original waves')\n",
    "plt.plot(WAVES_S.time, WAVES_S.Hs, label='Hs from storms')\n",
    "plt.plot(xds_mutau_S.time_runup_max, xds_mutau_S.Hs, '*r', label='Hs (TWLmax) hydrographs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlim(datetime(1997,9,1),datetime(1997,9,30))\n",
    "plt.title('South profile. pS = ' + str(pS))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WT.time, WT.sorted_bmus,'.', label='WT original')\n",
    "plt.plot(xds_mutau_S.time, xds_mutau_S.WT-0.3,'ok', label='WT hydrographs ini')\n",
    "plt.plot(xds_mutau_S.time_runup_max, xds_mutau_S.WT-0.55,'or', label='WT hydrographs max')\n",
    "plt.xlim(datetime(1997,9,1),datetime(1997,9,30))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot Tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot S (1 yr)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_S_h.time, WAVES_S_h.tp, label='Tp original waves')\n",
    "plt.plot(WAVES_S.time, WAVES_S.Tp, color='C1',label='Tp h from storms')\n",
    "plt.legend()\n",
    "plt.xlim(datetime(1997,1,1),datetime(1998,1,1))\n",
    "plt.title('South profile. pS = ' + str(pS))\n",
    "\n",
    "\n",
    "# plot South profile & WTs (1 month zoom)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(WAVES_S_h.time, WAVES_S_h.tp, label='Tp original waves')\n",
    "plt.plot(WAVES_S.time, WAVES_S.Tp, label='Tp from storms')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlim(datetime(1997,9,1),datetime(1997,9,30))\n",
    "plt.title('South profile. pS = ' + str(pS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot scatter (hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(WAVES_S_h) # 3-h\n",
    "print(WAVES_S) # 1-h\n",
    "print()\n",
    "\n",
    "WAVES_S_h = WAVES_S_h.resample(time='1H').interpolate('linear')\n",
    "WAVES_S_h = WAVES_S_h.sel(time=slice(WAVES_S.time[0],WAVES_S.time[-1]))\n",
    "print(WAVES_S_h) # 1-h\n",
    "print(WAVES_S) # 1-h\n",
    "print()\n",
    "\n",
    "var1 = 'hs'\n",
    "var2 = 'Hs'\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(WAVES_S_h[var1], WAVES_S[var2], '.')\n",
    "plt.xlabel(var2 + ' original waves')\n",
    "plt.ylabel(var2 + ' from storms')\n",
    "plt.xlim(-0.5, 4)\n",
    "plt.ylim(-0.5, 4)\n",
    "plt.title('hourly ' + var2)\n",
    "\n",
    "var1 = 'tp'\n",
    "var2 = 'Tp'\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(WAVES_S_h[var1], WAVES_S[var2], '.')\n",
    "plt.xlabel(var2 + ' original waves')\n",
    "plt.ylabel(var2 + ' from storms')\n",
    "plt.xlim(0, 22)\n",
    "plt.ylim(0, 22)\n",
    "plt.title('hourly ' + var2)\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot scatter (3-hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(WAVES_S_h) # 3-h\n",
    "print(WAVES_S) # 1-h\n",
    "print()\n",
    "\n",
    "WAVES_S_h = WAVES_S_h.sel(time=slice(WAVES_S.time[0],WAVES_S.time[-1]))\n",
    "WAVES_S = WAVES_S.sel(time=WAVES_S_h.time)\n",
    "print(WAVES_S_h) # 3-h\n",
    "print(WAVES_S) # 3-h\n",
    "print()\n",
    "\n",
    "var1 = 'hs'\n",
    "var2 = 'Hs'\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(WAVES_S_h[var1], WAVES_S[var2], '.')\n",
    "plt.xlabel(var2 + ' original waves')\n",
    "plt.ylabel(var2 + ' from storms')\n",
    "plt.xlim(-0.5, 4)\n",
    "plt.ylim(-0.5, 4)\n",
    "plt.title('3-hourly ' + var2)\n",
    "\n",
    "var1 = 'tp'\n",
    "var2 = 'Tp'\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(WAVES_S_h[var1], WAVES_S[var2], '.')\n",
    "plt.xlabel(var2 + ' original waves')\n",
    "plt.ylabel(var2 + ' from storms')\n",
    "plt.xlim(0, 22)\n",
    "plt.ylim(0, 22)\n",
    "plt.title('3-hourly ' + var2)\n",
    "\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hycreww RBF Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set parameters for Hycreww: TWL, Reef Slope, Beach Slope, Reef Width, Zbeach, Cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dset_N = xr.Dataset(\n",
    "            {   'hs':(('time'), WAVES_N.Hs.values),\n",
    "                'tp':(('time'), WAVES_N.Tp.values),\n",
    "                'bslope':(('time'), np.ones(len(WAVES_N.time),)*reef_N['Beach Slope'].values),\n",
    "                'rslope':(('time'), np.ones(len(WAVES_N.time),)*reef_N['Fore Slope'].values),\n",
    "                'rwidth':(('time'), np.ones(len(WAVES_N.time),)*reef_N['Wreef'].values),\n",
    "                'Zb':(('time'), np.ones(len(WAVES_N.time),)*Zbeach_N['zBeach'].values),\n",
    "                'cf':(('time'), np.ones(len(WAVES_N.time),)*0.1),\n",
    "            },coords = {'time': WAVES_N.time.values})\n",
    "\n",
    "dset_S = xr.Dataset(\n",
    "            {   'hs':(('time'), WAVES_S.Hs.values),\n",
    "                'tp':(('time'), WAVES_S.Tp.values),\n",
    "                'bslope':(('time'), np.ones(len(WAVES_S.time),)*reef_S['Beach Slope'].values),\n",
    "                'rslope':(('time'), np.ones(len(WAVES_S.time),)*reef_S['Fore Slope'].values),\n",
    "                'rwidth':(('time'), np.ones(len(WAVES_S.time),)*reef_S['Wreef'].values),\n",
    "                'Zb':(('time'), np.ones(len(WAVES_S.time),)*Zbeach_S['zBeach'].values),\n",
    "                'cf':(('time'), np.ones(len(WAVES_S.time),)*0.1),\n",
    "            },coords = {'time': WAVES_S.time.values})\n",
    "\n",
    "dset_E = xr.Dataset(\n",
    "            {   'hs':(('time'), WAVES_E.Hs.values),\n",
    "                'tp':(('time'), WAVES_E.Tp.values),\n",
    "                'bslope':(('time'), np.ones(len(WAVES_E.time),)*reef_E['Beach Slope'].values),\n",
    "                'rslope':(('time'), np.ones(len(WAVES_E.time),)*reef_E['Fore Slope'].values),\n",
    "                'rwidth':(('time'), np.ones(len(WAVES_E.time),)*reef_E['Wreef'].values),\n",
    "                'Zb':(('time'), np.ones(len(WAVES_E.time),)*Zbeach_E['zBeach'].values),\n",
    "                'cf':(('time'), np.ones(len(WAVES_E.time),)*0.1),\n",
    "            },coords = {'time': WAVES_E.time.values})\n",
    "\n",
    "dset_N = dset_N.sel(time=slice(dset_N.time.values[0],hist_TWL.time.values[-1]))\n",
    "dset_S = dset_S.sel(time=slice(dset_N.time.values[0],hist_TWL.time.values[-1]))\n",
    "dset_E = dset_E.sel(time=slice(dset_N.time.values[0],hist_TWL.time.values[-1]))\n",
    "\n",
    "dset_N['level'] = hist_TWL.TWL.sel(time=dset_N.time)\n",
    "dset_S['level'] = hist_TWL.TWL.sel(time=dset_S.time)\n",
    "dset_E['level'] = hist_TWL.TWL.sel(time=dset_E.time)\n",
    "\n",
    "dset_N['hs_lo2'] = (dset_N.hs/(1.5613*dset_N.tp**2))\n",
    "dset_S['hs_lo2'] = (dset_S.hs/(1.5613*dset_S.tp**2))\n",
    "dset_E['hs_lo2'] = (dset_E.hs/(1.5613*dset_E.tp**2))\n",
    "\n",
    "print(dset_N)\n",
    "print(dset_S)\n",
    "print(dset_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust variables to fit within Hycreww limits\n",
    "\n",
    "for var in var_lims.keys():\n",
    "\n",
    "    dset_N[var] =  dset_N[var].where(dset_N[var]>=var_lims[var][0], var_lims[var][0] )\n",
    "    dset_N[var] =  dset_N[var].where(dset_N[var]<=var_lims[var][1], var_lims[var][1] )    \n",
    "    \n",
    "    dset_S[var] =  dset_S[var].where(dset_S[var]>=var_lims[var][0], var_lims[var][0] )\n",
    "    dset_S[var] =  dset_S[var].where(dset_S[var]<=var_lims[var][1], var_lims[var][1] )    \n",
    "    \n",
    "    dset_E[var] =  dset_E[var].where(dset_E[var]>=var_lims[var][0], var_lims[var][0] )\n",
    "    dset_E[var] =  dset_E[var].where(dset_E[var]<=var_lims[var][1], var_lims[var][1] )    \n",
    "    \n",
    "print(dset_N)\n",
    "print(dset_S)\n",
    "print(dset_E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyCReWW-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyCReWW Function\n",
    "\n",
    "def hycreww_Q(var_lims, rbf_coeffs, dset):\n",
    "    '''\n",
    "    Calculates RunUp using hycreww RBFs (level) and linear interpolation (Runup)\n",
    "    \n",
    "    var_lims   - hycreww variables min and max limits\n",
    "    rbf_coeffs - hycreww rbf coefficients\n",
    "    dset       - input dataset (WL, Hs, Tp, Reef Slope, Beach Slope, Reef Width, Zbeach, Cf )\n",
    "    \n",
    "    '''\n",
    "    # RBF wave conditions \n",
    "    rbf_hs = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5]\n",
    "    rbf_hs_lo = [0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05 ]\n",
    "    rbf_vns = ['level', 'rslope', 'bslope', 'rwidth', 'Zb', 'cf']\n",
    "\n",
    "    # RBF parameters\n",
    "    ix_sc = [0, 1, 2, 3, 4, 5]\n",
    "    ix_dr = []\n",
    "    minis = [var_lims[x][0] for x in rbf_vns]\n",
    "    maxis = [var_lims[x][1] for x in rbf_vns]\n",
    "\n",
    "\n",
    "    # discard data outside limits\n",
    "#     for vn in var_lims.keys():\n",
    "#         dset = dset.isel(num=np.where((dset[vn] >= var_lims[vn][0]) & (dset[vn] <= var_lims[vn][1]))[0])\n",
    "\n",
    "\n",
    "    # RBF dataset to interpolate\n",
    "    ds_in = dset[rbf_vns]\n",
    "    ds_in=([dset.level.values],[dset.rslope.values],[dset.bslope.values],[dset.rwidth.values],[dset.Zb.values],[dset.cf.values])\n",
    "    ds_in=np.transpose(ds_in)[:,0,:]\n",
    "\n",
    "    # normalize data\n",
    "    ds_nm ,_ ,_ = Normalize(ds_in, ix_sc, ix_dr, minis=minis, maxis=maxis)\n",
    "\n",
    "    # RBF interpolation (with all cases?)\n",
    "    ru_out = []\n",
    "    for rc in rbf_coeffs:\n",
    "        ro = RBF_Interpolation(rc['constant'], rc['coeff'], rc['nodes'], ds_nm.T)\n",
    "        ru_out.append(ro)\n",
    "    ru_z = np.array(ru_out)\n",
    "\n",
    "    # Q Linear interpolation (hs, hs_lo -> runup)\n",
    "    Q = []\n",
    "    for c  in range(len(dset.time)):\n",
    "        vq = griddata((rbf_hs, rbf_hs_lo), ru_z[:,c], (dset.hs.values[c], dset.hs_lo2.values[c]), method='linear')\n",
    "        Q.append(vq)\n",
    "    Q = np.array(Q)\n",
    "\n",
    "    # store runup alongside input data\n",
    "    dset_out = dset.copy()  \n",
    "    dset_out['Q'] = (('time'),Q)\n",
    "\n",
    "    return dset_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_sim_N = hycreww_Q(var_lims, rbf_coeffs, dset_N)\n",
    "print(out_sim_N)\n",
    "\n",
    "out_sim_S = hycreww_Q(var_lims, rbf_coeffs, dset_S)\n",
    "print(out_sim_S)\n",
    "\n",
    "out_sim_E = hycreww_Q(var_lims, rbf_coeffs, dset_E)\n",
    "print(out_sim_E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We undo the box-cox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "# inv_boxcox(y, lmbda) \n",
    "# y = (x**lmbda - 1) / lmbda  if lmbda != 0\n",
    "#     log(x)                  if lmbda == 0\n",
    "    \n",
    "out_sim_N['Qinv']=inv_boxcox(out_sim_N.Q.values, 0.2)\n",
    "out_sim_S['Qinv']=inv_boxcox(out_sim_S.Q.values, 0.2)\n",
    "out_sim_E['Qinv']=inv_boxcox(out_sim_E.Q.values, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_sim_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "out_sim_N.to_netcdf(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_N' + str(pN) + '.nc'))\n",
    "out_sim_S.to_netcdf(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_S' + str(pS) + '.nc'))\n",
    "out_sim_E.to_netcdf(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_E' + str(pE) + '.nc'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_sim_N = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_N' + str(pN) + '.nc'))\n",
    "out_sim_S = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_S' + str(pS) + '.nc'))\n",
    "out_sim_E = xr.open_dataset(op.join(p_data, 'sites/KWAJALEIN/HYCREWW/hist_Q_storms_E' + str(pE) + '.nc'))\n",
    "\n",
    "print(out_sim_N)\n",
    "print(out_sim_S)\n",
    "print(out_sim_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1, figsize=(20,10))\n",
    "\n",
    "axs[0].plot(out_sim_N.time, out_sim_N.Qinv, color='g', label='Q north')\n",
    "axs[1].plot(out_sim_S.time, out_sim_S.Qinv, color='b', label='Q South')\n",
    "axs[2].plot(out_sim_E.time, out_sim_E.Qinv, color='r', label='Q East')\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "plt.suptitle('Overtopping (l/s/m)')\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(20,10))\n",
    "\n",
    "axs[0].plot(out_sim_N.time, out_sim_N.Q, label='Q north')\n",
    "axs[1].plot(out_sim_S.time, out_sim_S.Q, label='Q South')\n",
    "axs[2].plot(out_sim_E.time, out_sim_E.Q, label='Q East')\n",
    "\n",
    "plt.suptitle('Overtopping (l/s/m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(out_sim_S.time, out_sim_S.Qinv, color='b', label='Q South')\n",
    "plt.legend()\n",
    "plt.legend()\n",
    "plt.title('Overtopping (l/s/m)')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(out_sim_S.time.values[0:24*30], out_sim_S.Qinv.values[0:24*30], color='b', label='Q South')\n",
    "plt.legend()\n",
    "plt.legend()\n",
    "plt.title('Overtopping (l/s/m)')\n",
    "\n",
    "\n",
    "# histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "bins_number = 40 \n",
    "bins = np.linspace(np.nanmin(out_sim_S.Qinv.values), np.nanmax(out_sim_S.Qinv.values), bins_number)\n",
    "\n",
    "n, _, _ = plt.hist(out_sim_S.Qinv.values, bins)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# percentiles\n",
    "q = np.linspace(0,100,41)\n",
    "perc = np.nanpercentile(out_sim_S.Qinv.values, q)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(perc, q)\n",
    "plt.plot(perc, q, '.')\n",
    "plt.xlabel('Q')\n",
    "plt.ylabel('%')\n",
    "plt.grid()\n",
    "\n",
    "print(q)\n",
    "print()\n",
    "print(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
